# -*- coding: utf-8 -*-
"""Chi task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FCr24sveq_Ej0QWiTuZRq0PkrIOxTfDm
"""

!pip install pyspark

!pip install findspark

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local[*]').appName('Basics').getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better

from pyspark import SparkConf
from pyspark.context import SparkContext
from pyspark.sql.context import SQLContext

sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

spark = SparkSession \
    .builder \
    .appName("how to read csv file") \
    .getOrCreate()

path = '/content/Covid Data.csv'
df = spark.read.option("delimiter", ";").option("header", True).csv(path)
df.show()

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from pyspark.sql import functions as F

# Define the age_group function
def age_group(num):
    if num < 18:
        return "Age 0-18"
    elif 19 <= num <= 29:
        return "Age 19-30"
    elif 30 <= num <= 49:
        return "Age 30-50"
    elif 50 <= num <= 64:
        return "Age 50-65"
    elif 65 <= num <= 74:
        return "Age 65-75"
    else:
        return "Age 75+"

age_group_udf = udf(age_group, StringType())

df = df.withColumn('AGE', F.col('AGE').cast('int'))

df = df.withColumn('AGE GROUP', age_group_udf(df['AGE']))
df.select('AGE', 'AGE GROUP').show()

from pyspark.sql import functions as f

# Create a list of expressions to count nulls for each column
null_counts = [f.count(f.when(f.col(c) == 97, c)).alias(c) for c in df.columns]

# Apply the aggregation to count nulls for each column
df.select(null_counts).show(truncate=False)

df1 = df.rdd
df1.count()

#Total cases infected by covid
df2 = df1.filter(lambda row: int(row['CLASIFFICATION_FINAL']) >= 1 and int(row['CLASIFFICATION_FINAL']) <= 3)
total_cases = df2.count()

# Among all medical problems, which medical issues would get the highest case of Covid
def map_conditions(row):
    conditions = {
        'DIABETES': int(row['DIABETES']),
        'PNEUMONIA': int(row['PNEUMONIA']),
        'COPD': int(row['COPD']),
        'HIPERTENSION': int(row['HIPERTENSION']),
        'ASTHMA': int(row['ASTHMA']),
        'CARDIOVASCULAR': int(row['CARDIOVASCULAR']),
        'OBESITY': int(row['OBESITY']),
        'RENAL_CHRONIC': int(row['RENAL_CHRONIC']),
        'PREGNANT' : int(row['PREGNANT'])}

    return [(condition, 1) for condition, count in conditions.items() if count == 1]

medical_issue = df2.flatMap(map_conditions)
medical = medical_issue.groupByKey()
medical1 = medical.mapValues(sum)

for condition, count in medical1.collect():
    print(f"{condition}: {count} {round(count / total_cases,2)}")

# Out of the total cases, how many cases belong to these age groups (0-18, 18-30, 50-65, 65-75 and 75+ years.)
age_map = df2.map(lambda row: (row['AGE GROUP'], 1))
age_count = age_map.reduceByKey(lambda a, b: a + b)

for group, count in age_count.collect():
    print(f"{group}: {count} {round(count / total_cases, 2)}")

#

"""People carrying pneumonia have the highest probability of getting infected with Covid, which accounts for 22%, followed by those having hipertension (20%), diabetes (16%) and obesity (19%). Regarding age, people from 30-50 are highly infected with 44% of chance, while the age group from 50-65 gets 25% infection and the 19-30 age is 15%. People below 18 and over 75 are less likely to get Covide, taking 3% and 5% respectively"""

# How many people are diagnosed from level 1 to 3 and hospitalized
df4 = df1.filter(lambda row: int(row['CLASIFFICATION_FINAL']) >= 1 and int(row['CLASIFFICATION_FINAL']) <= 3 and int(row['PATIENT_TYPE']) == 2)
hospitalization = df4.count()

# The age group has the highest risk of being hospitalized (0-18, 18-29, 50–64, 65–74, 75–84 years, and 85+ years.)
age_map2 = df4.map(lambda row: (row['AGE GROUP'], 1))
age_count2 = age_map2.reduceByKey(lambda a, b: a + b)

for group, count in age_count2.collect():
    print(f"{group}: {count} {round(count / hospitalization, 2)}")

print(round(13736/20825, 2)) #probability of people over 75 need to hospitalize out of the total age-75+ cases
print(round(20597/32732, 2)) #probability of people over 75 need to hospitalize out of the total age-65-75 cases

"""The demographic of those aged 65-75 constitutes 8%, while those above 75 represent 5%. Nonetheless, it exists a 62-66% probability that those aged over 65 will require hospitalization. The probability for the middle-aged demographic is substantially increased, 28% for those in their 30s to 50s and 36% for those aged 50 to 65. The probability of hospitalization for people below 30 is relatively low, ranging from 1% to 4%."""

# How many cases are hospitalized that need to be in ICU
icu_condition = df4.filter(lambda row: int(row['PATIENT_TYPE']) == 2 and int(row['ICU']) == 1)
icu = icu_condition.count()

# Which age groups have the highest risk to be in ICU
icu_patient = icu_condition.map(lambda row: (row['AGE GROUP'], 1))
icu_group = icu_patient.reduceByKey(lambda a,b: a + b)
sort_group = sorted(icu_group.collect(), key=lambda x: x[1], reverse=True)

for group, count in sort_group:
    print(f"{group}: {count} {round(count / icu, 2)}")

# Define the cases are in ICU if they carried any medical problems
medical_pairs = icu_condition.flatMap(map_conditions)
medical_group = medical_pairs.groupByKey()
medical_group1 = medical_group.mapValues(sum)

for condition, count in medical_group1.collect():
    print(f"{condition}: {count} {round(count / icu,2)}")

#Dive in medical problems of the three groups, which has the most cases in ICU
age_medical = icu_condition.filter(lambda row: row['AGE GROUP'] == 'Age 50-65' and row['ICU'] == '1')
age_medical1 = age_medical.flatMap(map_conditions)
age_medical_group = age_medical1.groupByKey()
age_medical_group1 = age_medical_group.mapValues(sum)

for condition, count in age_medical_group1.collect():
    print(f"{condition}: {count} {round(count/age_medical.count(), 2)}")

elderly_medical = icu_condition.filter(lambda row: row['AGE GROUP'] == 'Age 65-75' or row['AGE GROUP'] == 'Age 75+')
elderly_medical1 = elderly_medical.flatMap(map_conditions)
elderly_group = elderly_medical1.groupByKey()
elderly_group1 = elderly_group.mapValues(sum)

for condition, count in elderly_group1.collect():
    print(f"{condition}: {count} {round(count/elderly_medical.count(), 2)}")

middleage_medical = icu_condition.filter(lambda row: row['AGE GROUP'] == 'Age 30-50')
middleage_medical1 = middleage_medical.flatMap(map_conditions)
middleage_group = middleage_medical1.groupByKey()
middleage_group1 = middleage_group.mapValues(sum)

for condition, count in middleage_group1.collect():
    print(f"{condition}: {count} {round(count/middleage_medical.count(), 2)}")

"""Patients over 30 years old have a significant incidence of being in the ICU, particularly those aged 50-65, with a 36% chances, 30% for age over 65 and 28% for those aged 30-50. The analysis indicated that people who carry pneumonia, hipertension, obesity and diabetes have a high potential rate from the virus. The ICU statistics represent the patients with intensive care mainly those having pre-conditions, accounting for 87% of pneumonia, 35% of hipertension, 32% of diabetes, and 28% of obesity.

The research analyses the instances of high potential COVID-19 infection among age groups and medical conditions based on probabilities. However, it provided little information on the prediction of patients with certain conditions prioritized for ICU. The predictive model is designed to assist in medical diagnosis and treatment.
"""

# A predictive model based on the pre-existing medical features
from pyspark.sql.functions import col, when, trim
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.sql.types import IntegerType

# Add the NEED_ICU column based on the ICU status
df = df.withColumn("NEED_ICU", when(col("ICU") == "1", 1).otherwise(0))

# Create the RAS_SICK column based on multiple medical conditions
df = df.withColumn(
    "CONDITIONS",
    when(
        (col("COPD") == 1) | (col("ASTHMA") == 1) | (col("CARDIOVASCULAR") == 1) |
        (col("RENAL_CHRONIC") == 1) | (col("PNEUMONIA") == 1) |
        (col("PREGNANT") == 1) | (col("OBESITY") == 1) |
        (col("HIPERTENSION") == 1) | (col("DIABETES") == 1), 1
    ).when(
        (col("COPD") == 2) | (col("ASTHMA") == 2) | (col("CARDIOVASCULAR") == 2) |
        (col("RENAL_CHRONIC") == 2) | (col("PNEUMONIA") == 2) |
        (col("PREGNANT") == 2) | (col("OBESITY") == 2) |
        (col("HIPERTENSION") == 2) | (col("DIABETES") == 2), 0
    ).otherwise(None))

# Filter out rows where age or medical conditions are missing
df_filtered = df.filter(df.CONDITIONS.isNotNull())

# Assemble the features into a single vector
assembler = VectorAssembler(inputCols=["CONDITIONS"], outputCol="features")
df_assembled = assembler.transform(df_filtered)

df_final = df_assembled.select("features", col("NEED_ICU").alias("label"))

# Split the data into training (70%), validation (15%), and test (15%) sets
train_data, val_data, test_data = df_final.randomSplit([0.7, 0.15, 0.15], seed=27)

# Fit the logistic regression model on the training data
lr = LogisticRegression()
lr_model = lr.fit(train_data)

# Evaluate the validation data
val_predictions = lr_model.transform(val_data)
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="label")
val_auc = evaluator.evaluate(val_predictions)
print(f"Validation AUC: {val_auc}")

# Make predictions on the test data
test_predictions = lr_model.transform(test_data)

# Evaluate the model on the test set
test_auc = evaluator.evaluate(test_predictions)
print(f"Test AUC: {test_auc}")

# Predictive model based on age
from pyspark.sql.functions import col, when
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Assuming `df` is your DataFrame containing 'AGE' and 'ICU' columns

# Step 1: Encode ICU need into binary (1 for ICU needed, 0 otherwise)
df = df.withColumn("NEED_ICU", when(col("ICU") == "1", 1).otherwise(0))

# Step 2: Convert the 'AGE' column to a numeric type (if it isn't already)
df = df.withColumn("AGE", col("AGE").cast("integer"))

# Step 3: Select only the relevant columns (AGE and NEED_ICU)
df_filtered1 = df.select("AGE", "NEED_ICU").filter(df.AGE.isNotNull())

# Step 4: Assemble the 'AGE' feature into a feature vector
assembler1 = VectorAssembler(inputCols=["AGE"], outputCol="features")
df_assembled1 = assembler1.transform(df_filtered1)

# Step 5: Select the feature vector and the target label (NEED_ICU)
df_final1 = df_assembled1.select("features", col("NEED_ICU").alias("label"))

# Step 6: Split the data into training (70%) and test (30%) sets
train_data, test_data = df_final1.randomSplit([0.7, 0.3], seed=27)

# Step 7: Fit the logistic regression model on the training data
lr1 = LogisticRegression()
lr_model1 = lr1.fit(train_data)

# Step 8: Make predictions on the test data
test_predictions1 = lr_model.transform(test_data)

# Step 9: Evaluate the model on the test data using AUC
evaluator1 = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="label")
test_auc1 = evaluator1.evaluate(test_predictions1)
print(f"Test AUC: {test_auc}")

# NOTE: Because of the exponential growth, it is recommended to run two predictive models in another file.